{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fdad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based upon the 'Build a Comment Toxicity Model with Deep Learning and Python' tutorial by Nicholas Renotte\n",
    "# Link: https://www.youtube.com/watch?v=ZUqB-luawZg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef0e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrading Pip Installer\n",
    "#!pip3 install --upgrade pip # Only for non 'mlp' environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Dependencies\n",
    "#!pip3 install tensorflow # Only for non 'mlp' environment\n",
    "#!pip3 install tensorflow-gpu # Only for non 'mlp' environment\n",
    "#!pip3 install pandas\n",
    "#!pip3 install matplotlib\n",
    "#!pip3 install sklearn\n",
    "#!pip3 install gradio jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fa1d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Availalbe Pip Libraries\n",
    "#!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaab2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available Conda Libraries\n",
    "#!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9979015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "import gradio as gr\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# For graphical representations\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Layers used to build up a deep neural network\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# For determining the accuracy of the model\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from tensorflow.keras.metrics import Recall\n",
    "\n",
    "# One of many ways to create a model, easiest and quickest in this scenario\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data\n",
    "\n",
    "# 'os.path.join' presents the full file path to the desired data file\n",
    "dfComments = pd.read_csv('../../Data/train.csv')\n",
    "#dfComments = pd.read_csv(os.path.join('/Users', 'username', 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec076903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying that the data has been properly loaded in\n",
    "dfComments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tail end of the dataframe\n",
    "dfComments.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the contents of the very first comment\n",
    "dfComments.iloc[0]['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269231b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding some toxic comments\n",
    "dfComments[dfComments['toxic'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e517cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the toxicity value of a comment\n",
    "dfComments[dfComments.columns[2:]].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2025e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing The Data\n",
    "\n",
    "# Splitting the dataset into its comments and their overall features\n",
    "commentsText = dfComments['comment_text']\n",
    "\n",
    "# Transforming via '.values' into a numpy array\n",
    "# Each comment text will have a vector defining as to which sort of toxicity \n",
    "## categories they may or may not fall into\n",
    "commentsFeatures = dfComments[dfComments.columns[2:]].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfComments.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(commentsText.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(commentsFeatures[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e937d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the maximum number of words allowed within the vocabulary model's dictionary\n",
    "## The greater the number, the larger the model\n",
    "## Each word will be tokenized to a unique number so as to be able to identify it\n",
    "MAXWORDS = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing up documentation\n",
    "TextVectorization??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the text vectorization layer\n",
    "## Lowers and removes punctuation\n",
    "## 'max_tokens': The maximum number of words allowed in terms of vocabulary\n",
    "## 'output_sequence_length': The maximum allowed input length of a sentence\n",
    "## 'output_mode': In what format is the word supposed to be map to a number, in this case an integer value\n",
    "vectorizer = TextVectorization(max_tokens = MAXWORDS,\n",
    "                               output_sequence_length = 1800,\n",
    "                               output_mode = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer is learning each and every word up to the pre-set maximum within the given comments' text\n",
    "## The pandas series, essential a signle column file format, is transformed into a numpy array via '.values'\n",
    "print(type(commentsText))\n",
    "print(type(commentsText.values))\n",
    "vectorizer.adapt(commentsText.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorizer can now transform a passed in sentence passed upon its now established dictionary\n",
    "print(vectorizer('Today was a great day.')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vectorized version of all of the comments' text data with the now adapted vectorizer\n",
    "vectorizedCommentsText = vectorizer(commentsText.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the lengths of the original and vectorized comments' text data\n",
    "print(len(commentsText))\n",
    "\n",
    "# If a sentence was shorter than the 1800 limit word length, than the \n",
    "## unused columns are set to a value of 0 via padding\n",
    "vectorizedCommentsText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TensorFlow data pipeline\n",
    "## Particularly useful when you have a quantity of data which cannot simply \n",
    "### be simultaneously brought into memeory\n",
    "## MCSHBAP Acronym: Map, Cache, Shuffle, Batch, Prefetch\n",
    "### Basic data pipeline generation steps\n",
    "\n",
    "### Initiated either by the 'from_tensor_slices' or 'list_files' method\n",
    "## 'vectorizedCommentsText': The 'x' value or input features\n",
    "## 'commentFeatures': The 'y' value or target\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorizedCommentsText, commentsFeatures))\n",
    "\n",
    "# Chaching the data\n",
    "dataset = dataset.cache()\n",
    "\n",
    "# Shuffling the data with a passed in buffer size\n",
    "dataset = dataset.shuffle(160000)\n",
    "\n",
    "# Batching the dataset as a series of 16 samples\n",
    "dataset = dataset.batch(16)\n",
    "\n",
    "# Helping to prevent bottlenecking\n",
    "dataset = dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66200ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving one batch of the above data pipeline\n",
    "## Represented as the vectorized comments' text along with their associated feature labels\n",
    "batchX, batchY = dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector of value sets\n",
    "batchX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3880045",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cf6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656575bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length of the dataset in batches\n",
    "print(len(dataset))\n",
    "\n",
    "# Partitioning the dataset into the testing, validation and training datasets\n",
    "## Can extract these partitions directly from the data pipeline\n",
    "## Rounding percentages of partitioning process to an integer value\n",
    "## Remembering to skip over those portions already partitioned to not reuse duplicate data and forget the leftovers\n",
    "train = dataset.take(int(len(dataset) * 0.7))\n",
    "validation = dataset.skip(int(len(dataset) * 0.7)).take(int(len(dataset) * 0.2))\n",
    "test = dataset.take(int(len(dataset) * 0.9)).take(int(len(dataset) * 0.1))\n",
    "\n",
    "# Forward pass, backwards pass, upgrade the gradient\n",
    "\n",
    "print('Batches Per Partition: ')\n",
    "print(f'Training: {len(train)}')\n",
    "print(f'Validation: {len(validation)}')\n",
    "print(f'Testing: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e06c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating A Sequential Model\n",
    "# Model sequential API being instantiated\n",
    "model = Sequential()\n",
    "\n",
    "# Adding Layers\n",
    "\n",
    "# Sequences passed down into the embedding layer, acting as a sort of personality test per word, learned as \n",
    "## the deep neural network is trained, no need to pass in a pre-existing embedding\n",
    "# Number of words + 1 do to unknown words being embedded as a whole\n",
    "# 1 Embedding per word, each being 32 feature values in length\n",
    "model.add(Embedding(MAXWORDS + 1, 32))\n",
    "\n",
    "# Bidirectional wrapper for neural layer networks, very important for Natural Language Processing (NLP)\n",
    "## Allows for information to be passed in both directions as opposed to the default 1\n",
    "## Important due to how previous words in a sentence can affect meaning of following words\n",
    "### Example: \"I don't hate you.\" \n",
    "### If this was read only from left to right, the \"don't\" would not affect the interpretation of the word 'hate'\n",
    "### as a toxic value, as opposed to if it can be read from right to left as well\n",
    "# GPU acceleration needed for an LSTM layer is an activation of 'tanh' as dictated by tensorflow\n",
    "model.add(Bidirectional(LSTM(32, activation = 'tanh')))\n",
    "\n",
    "# Feature extractor dense fully connected layers\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "# Final layer\n",
    "## Maps to the 6 different outputs possible in terms of the target toxicity\n",
    "model.add(Dense(6, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a414aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(commentsFeatures.shape)\n",
    "print(commentsFeatures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "# Using binary crossentropy as opposed to categorical as each output is it's own independent feature\n",
    "## and not part of a whole, this is essentially a multi output model\n",
    "model.compile(loss = 'BinaryCrossentropy', optimizer = 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4587be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional doubles the number of units within the LSTM layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4212b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the deep neural network model\n",
    "## Passing in the training data, how long the desire to train and the validation data\n",
    "## Loss should be progressively decreasing as training nears completion\n",
    "# history = model.fit(train, epochs = 10, validation_data = validation)\n",
    "model.fit(train, epochs = 10, validation_data = validation)\n",
    "\n",
    "# Computer Specifications:\n",
    "# MacOS Monterey: Version 12.4\n",
    "# Chip: Apple M1 Max, 10 Cores (8 Performance & 2 Efficency), GPU 32 Cores\n",
    "# Memory: 64 GB\n",
    "\n",
    "# Epoch Training:\n",
    "# Epoch 1: Time = 7949s, Loss = 0.0619, Validation Loss = 0.0443\n",
    "# Epoch 2: Time = 7913s, Loss = 0.0456, Validation Loss = 0.0388\n",
    "# Epoch 3: Time = 7862s, Loss = 0.0408, Validation Loss = 0.0356\n",
    "# Epoch 4: Time = 7982s, Loss = 0.0357, Validation Loss = 0.0320\n",
    "# Epoch 5: Time = 7901s, Loss = 0.0326, Validation Loss = 0.0283\n",
    "# Epoch 6: Time = 7873s, Loss = 0.0297, Validation Loss = 0.0256\n",
    "# Epoch 7: Time = 7899s, Loss = 0.0267, Validation Loss = 0.0232\n",
    "# Epoch 8: Time = 7909s, Loss = 0.0238, Validation Loss = 0.0202\n",
    "# Epoch 9: Time = 7964s, Loss = 0.0215, Validation Loss = 0.0201\n",
    "# Epoch 10: Time = 8056s, Loss = 0.0199, Validation Loss = 0.0171\n",
    "\n",
    "# Total Training Time: 79308s =  22 Hours,  1 Minutes,  48 Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa461050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss metrics of model training\n",
    "#history.history\n",
    "epochs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "training_losses = [0.0619, 0.0456, 0.0408, 0.0357, 0.0326, 0.0297, 0.0267, 0.0238, 0.0215, 0.0199]\n",
    "validation_losses = [0.0443, 0.0388, 0.0356, 0.0320, 0.0283, 0.0256, 0.0232, 0.0202, 0.0201, 0.0171]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ede48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting change in losses over training epochs\n",
    "\n",
    "#plt.figure(figsize = (8, 5))\n",
    "#pd.DataFrame(history.history).plot()\n",
    "\n",
    "plt.plot(epochs, training_losses, color = 'blue', ls = '-', marker = '.', label = 'Training Loss')\n",
    "plt.plot(epochs, validation_losses, color = 'black', ls = '-', marker = '.', label = 'Validation Loss')\n",
    "plt.xlabel('Training Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Training & Validation Loss Over Training Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "# The sample text has to be vectorized and the words tokenized for the model to be able to make sense of it\n",
    "sampleInputText = vectorizer('You freaking suck.')\n",
    "\n",
    "# A sequence of integers\n",
    "print(sampleInputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model expects a series of values or a batch, not a single value, so the vectorized \n",
    "## sample input has to be wrapped within a numpy array\n",
    "## The input shape for the model and the passed in value must match\n",
    "print(np.array([sampleInputText]))\n",
    "print(np.expand_dims(sampleInputText, 0))\n",
    "\n",
    "#model.predict(np.array([sampleInputText]))\n",
    "\n",
    "# Cleaner version than above example\n",
    "sampleResult = model.predict(np.expand_dims(sampleInputText, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = test.as_numpy_iterator().next()\n",
    "batchX, batchY = test.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.predict(batchX) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating The Model\n",
    "# Allows for aggregation of metrics over time as the measurement are iterated over and over again over batches\n",
    "modelPrecision = Precision()\n",
    "modelRecall = Recall()\n",
    "modelCategoricalAccuracy = CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through every single batch\n",
    "for batch in test.as_numpy_iterator():\n",
    "    \n",
    "    # Unpacking the batch\n",
    "    xTrue, yTrue = batch\n",
    "    \n",
    "    # Making the precition\n",
    "    yHat = model.predict(xTrue)\n",
    "    \n",
    "    # Flattening the predictions into one very large vector\n",
    "    ## Instead of a 6x6 matrix, it becomes a 36x1\n",
    "    yTrue = yTrue.flatten()\n",
    "    yHat = yHat.flatten()\n",
    "    \n",
    "    # Calculating the metrics for the batch, then updating the existing Key Performance Indicators (KPIs)\n",
    "    modelPrecision.update_state(yTrue, yHat)\n",
    "    modelRecall.update_state(yTrue, yHat)\n",
    "    modelCategoricalAccuracy.update_state(yTrue, yHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prinintg Results\n",
    "print(f'Precision: {modelPrecision.result().numpy()}, Recall: {modelRecall.result().numpy()}, Accuracy: {modelCategoricalAccuracy.result().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving The Model\n",
    "print(os.getcwd())\n",
    "\n",
    "os.chdir('../../Models')\n",
    "print(os.getcwd())\n",
    "\n",
    "model.save('commentToxicityModel.h5')\n",
    "print(os.listdir(os.getcwd()))\n",
    "\n",
    "os.chdir('../Python/PythonMachineLearning')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f71acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading The Model\n",
    "model = tf.keras.models.load_model('commentToxicityModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2718af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing & Gradio Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to be connected to Gradio\n",
    "\n",
    "# A text comment is passed in as an argument\n",
    "def scoreCommentToxicity(commentText):\n",
    "    \n",
    "    # The comment is passed through the vectorizer, tokenizing the words\n",
    "    # Text is converted into a sequence of numbers\n",
    "    vectorizedComment = vectorizer([commentText])\n",
    "    \n",
    "    # The tokenized comment is passed through the model to predict whether or not it is of a toxic nature\n",
    "    results = model.predict(vectorizedComment)\n",
    "    \n",
    "    text = ''\n",
    "    \n",
    "    # Unpacks the 'results' by looping through each of the dataframe's columns\n",
    "    # Prints out whether or not the column's feature is true or false for its associated comment\n",
    "    for index, column in enumerate(dfComments.columns[2:]):\n",
    "        text += '{}: {}\\n'.format(column, results[0][index] > 0.5)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating The Gradio Interface\n",
    "## 'fn': The function the Gradio interface makes use of\n",
    "## 'inputs': The type of input\n",
    "## 'outputs': The style of output\n",
    "gradioInterface = gr.Interface(fn = scoreCommentToxicity,\n",
    "                               inputs = gr.inputs.Textbox(lines = 2, placeholder = 'Comment to be scored.'),\n",
    "                               outputs = 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a60ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Launching The Interface\n",
    "## If the 'share' value is set to 'True', the gradio application will be made public for a limited amount of time\n",
    "#gradioInterface.launch(share = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreCommentToxicity('I really hate you and I am going to attack you fucker.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
