{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef0e1de",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (22.0.4)\n",
      "Collecting pip\n",
      "  Downloading pip-22.1.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.0.4\n",
      "    Uninstalling pip-22.0.4:\n",
      "      Successfully uninstalled pip-22.0.4\n",
      "Successfully installed pip-22.1.2\n"
     ]
    }
   ],
   "source": [
    "# Upgrading Pip Installer\n",
    "#!pip3 install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1317b910",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-3.0.18-py3-none-any.whl (5.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.1.2)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp310-cp310-macosx_11_0_arm64.whl (552 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.5/552.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading fastapi-0.78.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (1.4.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (1.22.4)\n",
      "Collecting analytics-python\n",
      "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (9.1.1)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting paramiko\n",
      "  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pycryptodome\n",
      "  Downloading pycryptodome-3.14.1.tar.gz (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-multipart\n",
      "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting uvicorn\n",
      "  Downloading uvicorn-0.17.6-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown-it-py[linkify,plugins]\n",
      "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (3.5.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gradio) (2.28.0)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.7.2-cp310-cp310-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (447 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.0/447.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2) (2.1.1)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->gradio) (21.4.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp310-cp310-macosx_11_0_arm64.whl (34 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp->gradio) (2.0.12)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp310-cp310-macosx_11_0_arm64.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting backoff==1.10.0\n",
      "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from analytics-python->gradio) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from analytics-python->gradio) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->gradio) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->gradio) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests->gradio) (1.26.9)\n",
      "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
      "  Downloading pydantic-1.9.1-cp310-cp310-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting starlette==0.19.1\n",
      "  Downloading starlette-0.19.1-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
      "  Using cached anyio-3.6.1-py3-none-any.whl (80 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting linkify-it-py~=1.0\n",
      "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
      "Collecting mdit-py-plugins\n",
      "  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->gradio) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->gradio) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->gradio) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib->gradio) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas->gradio) (2022.1)\n",
      "Collecting pynacl>=1.0.1\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-macosx_10_10_universal2.whl (349 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.9/349.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=2.5\n",
      "  Downloading cryptography-37.0.2-cp36-abi3-macosx_10_10_universal2.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting bcrypt>=3.1.3\n",
      "  Downloading bcrypt-3.2.2-cp36-abi3-macosx_10_10_universal2.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from uvicorn->gradio) (8.1.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/58.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting asgiref>=3.4.0\n",
      "  Downloading asgiref-3.5.2-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: cffi>=1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from bcrypt>=3.1.3->paramiko->gradio) (1.15.0)\n",
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi->gradio) (4.2.0)\n",
      "Collecting sniffio>=1.1\n",
      "  Using cached sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio) (2.21)\n",
      "Using legacy 'setup.py install' for ffmpy, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for pycryptodome, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for python-multipart, since package 'wheel' is not installed.\n",
      "Installing collected packages: pydub, monotonic, ffmpy, uc-micro-py, sniffio, python-multipart, pydantic, pycryptodome, orjson, multidict, mdurl, h11, fsspec, frozenlist, backoff, async-timeout, asgiref, yarl, uvicorn, pynacl, markdown-it-py, linkify-it-py, cryptography, bcrypt, anyio, analytics-python, aiosignal, starlette, paramiko, mdit-py-plugins, aiohttp, fastapi, gradio\n",
      "  Running setup.py install for ffmpy ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for python-multipart ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for pycryptodome ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed aiohttp-3.8.1 aiosignal-1.2.0 analytics-python-1.4.0 anyio-3.6.1 asgiref-3.5.2 async-timeout-4.0.2 backoff-1.10.0 bcrypt-3.2.2 cryptography-37.0.2 fastapi-0.78.0 ffmpy-0.3.0 frozenlist-1.3.0 fsspec-2022.5.0 gradio-3.0.18 h11-0.13.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.0 mdurl-0.1.1 monotonic-1.6 multidict-6.0.2 orjson-3.7.2 paramiko-2.11.0 pycryptodome-3.14.1 pydantic-1.9.1 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 sniffio-1.2.0 starlette-0.19.1 uc-micro-py-1.0.1 uvicorn-0.17.6 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "# Installing Dependencies\n",
    "#!pip3 install tensorflow\n",
    "#!pip3 install tensorflow-gpu\n",
    "#!pip3 install pandas\n",
    "#!pip3 install matplotlib\n",
    "#!pip3 install sklearn\n",
    "#!pip3 install gradio jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71fa1d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package              Version\r\n",
      "-------------------- ------------------\r\n",
      "appnope              0.1.3\r\n",
      "argon2-cffi          21.3.0\r\n",
      "argon2-cffi-bindings 21.2.0\r\n",
      "asttokens            2.0.5\r\n",
      "attrs                21.4.0\r\n",
      "backcall             0.2.0\r\n",
      "beautifulsoup4       4.11.1\r\n",
      "bleach               5.0.0\r\n",
      "certifi              2022.6.15\r\n",
      "cffi                 1.15.0\r\n",
      "charset-normalizer   2.0.12\r\n",
      "click                8.1.3\r\n",
      "cycler               0.11.0\r\n",
      "debugpy              1.6.0\r\n",
      "decorator            5.1.1\r\n",
      "defusedxml           0.7.1\r\n",
      "entrypoints          0.4\r\n",
      "executing            0.8.3\r\n",
      "fastjsonschema       2.15.3\r\n",
      "fonttools            4.33.3\r\n",
      "glob2                0.7\r\n",
      "idna                 3.3\r\n",
      "ipykernel            6.13.0\r\n",
      "ipython              8.3.0\r\n",
      "ipython-genutils     0.2.0\r\n",
      "jedi                 0.18.1\r\n",
      "Jinja2               3.1.2\r\n",
      "joblib               1.1.0\r\n",
      "jsonschema           4.5.1\r\n",
      "jupyter-client       7.3.1\r\n",
      "jupyter-core         4.10.0\r\n",
      "jupyterlab-pygments  0.2.2\r\n",
      "kiwisolver           1.4.2\r\n",
      "lxml                 4.9.0\r\n",
      "MarkupSafe           2.1.1\r\n",
      "matplotlib           3.5.2\r\n",
      "matplotlib-inline    0.1.3\r\n",
      "mistune              0.8.4\r\n",
      "mlxtend              0.20.0\r\n",
      "nbclient             0.6.3\r\n",
      "nbconvert            6.5.0\r\n",
      "nbformat             5.4.0\r\n",
      "nest-asyncio         1.5.5\r\n",
      "nltk                 3.7\r\n",
      "notebook             6.4.11\r\n",
      "numpy                1.22.4\r\n",
      "opencv-python        4.6.0.66\r\n",
      "packaging            21.3\r\n",
      "pandas               1.4.2\r\n",
      "pandocfilters        1.5.0\r\n",
      "parso                0.8.3\r\n",
      "pexpect              4.8.0\r\n",
      "pickleshare          0.7.5\r\n",
      "Pillow               9.1.1\r\n",
      "pip                  22.1.2\r\n",
      "prometheus-client    0.14.1\r\n",
      "prompt-toolkit       3.0.29\r\n",
      "psutil               5.9.1\r\n",
      "ptyprocess           0.7.0\r\n",
      "pure-eval            0.2.2\r\n",
      "pycparser            2.21\r\n",
      "Pygments             2.12.0\r\n",
      "pyparsing            3.0.9\r\n",
      "pyrsistent           0.18.1\r\n",
      "python-dateutil      2.8.2\r\n",
      "pytz                 2022.1\r\n",
      "pyzmq                23.0.0\r\n",
      "regex                2022.6.2\r\n",
      "requests             2.28.0\r\n",
      "scikit-learn         1.1.1\r\n",
      "scipy                1.8.1\r\n",
      "seaborn              0.11.2\r\n",
      "Send2Trash           1.8.0\r\n",
      "setuptools           58.1.0\r\n",
      "six                  1.16.0\r\n",
      "sklearn              0.0\r\n",
      "soupsieve            2.3.2.post1\r\n",
      "stack-data           0.2.0\r\n",
      "terminado            0.15.0\r\n",
      "threadpoolctl        3.1.0\r\n",
      "tinycss2             1.1.1\r\n",
      "torch                1.13.0.dev20220617\r\n",
      "torchaudio           0.14.0.dev20220603\r\n",
      "torchvision          0.14.0.dev20220617\r\n",
      "tornado              6.1\r\n",
      "tqdm                 4.64.0\r\n",
      "traitlets            5.2.1.post0\r\n",
      "typing_extensions    4.2.0\r\n",
      "urllib3              1.26.9\r\n",
      "wcwidth              0.2.5\r\n",
      "webencodings         0.5.1\r\n"
     ]
    }
   ],
   "source": [
    "# Availalbe Pip Libraries\n",
    "!pip3 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9979015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "import gradio as gr\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# For graphical representations\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Layers used to build up a deep neural network\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# For determining the accuracy of the model\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.keras.metrics import Precision\n",
    "from tensorflow.keras.metrics import Recall\n",
    "\n",
    "# One of many ways to create a model, easiest and quickest in this scenario\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae2bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data\n",
    "\n",
    "# 'os.path.join' presents the full file path to the desired data file\n",
    "dfComments = pd.read_csv(os.path.join('/Users', 'maxencebrette', 'Documents', 'Coding', 'Data', 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec076903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying that the data has been properly loaded in\n",
    "dfComments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f20e9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tail end of the dataframe\n",
    "dfComments.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c83a1764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving the contents of the very first comment\n",
    "dfComments.iloc[0]['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "269231b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "12  0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
       "16  0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
       "42  001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
       "43  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "6       1             1        1       0       1              0  \n",
       "12      1             0        0       0       0              0  \n",
       "16      1             0        0       0       0              0  \n",
       "42      1             0        1       0       1              1  \n",
       "43      1             0        1       0       1              0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding some toxic comments\n",
    "dfComments[dfComments['toxic'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41e517cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            1\n",
       "severe_toxic     1\n",
       "obscene          1\n",
       "threat           0\n",
       "insult           1\n",
       "identity_hate    0\n",
       "Name: 6, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving the toxicity value of a comment\n",
    "dfComments[dfComments.columns[2:]].iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2025e6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Processing The Data\n",
    "\n",
    "# Splitting the dataset into its comments and their overall features\n",
    "commentsText = dfComments['comment_text']\n",
    "\n",
    "# Transforming via '.values' into a numpy array\n",
    "# Each comment text will have a vector defining as to which sort of toxicity \n",
    "## categories they may or may not fall into\n",
    "commentsFeatures = dfComments[dfComments.columns[2:]].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ee4d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dfComments.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b691701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Explanation\\nWhy the edits made under my usern...\n",
      "1    D'aww! He matches this background colour I'm s...\n",
      "2    Hey man, I'm really not trying to edit war. It...\n",
      "3    \"\\nMore\\nI can't make any real suggestions on ...\n",
      "4    You, sir, are my hero. Any chance you remember...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(commentsText.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "918d6703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(commentsFeatures[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e937d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the maximum number of words allowed within the vocabulary model's dictionary\n",
    "## The greater the number, the larger the model\n",
    "## Each word will be tokenized to a unique number so as to be able to identify it\n",
    "MAXWORDS = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing up documentation\n",
    "TextVectorization??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the text vectorization layer\n",
    "## Lowers and removes punctuation\n",
    "## 'max_tokens': The maximum number of words allowed in terms of vocabulary\n",
    "## 'output_sequence_length': The maximum allowed input length of a sentence\n",
    "## 'output_mode': In what format is the word supposed to be map to a number, in this case an integer value\n",
    "vectorizer = TextVectorization(max_tokens = MAXWORDS,\n",
    "                               output_sequence_length = 1800,\n",
    "                               output_mode = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer is learning each and every word up to the pre-set maximum within the given comments' text\n",
    "## The pandas series, essential a signle column file format, is transformed into a numpy array via '.values'\n",
    "print(type(commentsText))\n",
    "print(type(commentsText.values))\n",
    "vectorizer.adapt(commentsText.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorizer can now transform a passed in sentence passed upon its now established dictionary\n",
    "print(vectorizer('Today was a great day.')[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a vectorized version of all of the comments' text data with the now adapted vectorizer\n",
    "vectorizedCommentsText = vectorizer(commentsText.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the lengths of the original and vectorized comments' text data\n",
    "print(len(commentsText))\n",
    "\n",
    "# If a sentence was shorter than the 1800 limit word length, than the \n",
    "## unused columns are set to a value of 0 via padding\n",
    "vectorizedCommentsText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TensorFlow data pipeline\n",
    "## Particularly useful when you have a quantity of data which cannot simply \n",
    "### be simultaneously brought into memeory\n",
    "## MCSHBAP Acronym: Map, Cache, Shuffle, Batch, Prefetch\n",
    "### Basic data pipeline generation steps\n",
    "\n",
    "### Initiated either by the 'from_tensor_slices' or 'list_files' method\n",
    "## 'vectorizedCommentsText': The 'x' value or input features\n",
    "## 'commentFeatures': The 'y' value or target\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorizedCommentsText, commentsFeatures))\n",
    "\n",
    "# Chaching the data\n",
    "dataset = dataset.chache()\n",
    "\n",
    "# Shuffling the data with a passed in buffer size\n",
    "dataset = dataset.shuffle(160000)\n",
    "\n",
    "# Batching the dataset as a series of 16 samples\n",
    "dataset = dataset.batch(16)\n",
    "\n",
    "# Helping to prevent bottlenecking\n",
    "dataset = dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66200ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving one batch of the above data pipeline\n",
    "## Represented as the vectorized comments' text along with their associated feature labels\n",
    "batchX, batchY = dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3270476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector of value sets\n",
    "batchX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3880045",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cf6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656575bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length of the dataset in batches\n",
    "print(len(dataset))\n",
    "\n",
    "# Partitioning the dataset into the testing, validation and training datasets\n",
    "## Can extract these partitions directly from the data pipeline\n",
    "## Rounding percentages of partitioning process to an integer value\n",
    "## Remembering to skip over those portions already partitioned to not reuse duplicate data and forget the leftovers\n",
    "train = dataset.take(int(len(dataset) * 0.7))\n",
    "validation = dataset.skip(int(len(dataset) * 0.7)).take(int(len(dataset) * 0.2))\n",
    "test = dataset.take(int(len(dataset) * 0.9)).take(int(len(dataset) * 0.1))\n",
    "\n",
    "# Forward pass, backwards pass, upgrade the gradient\n",
    "\n",
    "print('Batches Per Partition: ')\n",
    "print(f'Training: {len(train)}')\n",
    "print(f'Validation: {len(validation)}')\n",
    "print(f'Testing: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e06c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating A Sequential Model\n",
    "# Model sequential API being instantiated\n",
    "model = Sequential\n",
    "\n",
    "# Adding Layers\n",
    "\n",
    "# Sequences passed down into the embedding layer, acting as a sort of personality test per word, learned as \n",
    "## the deep neural network is trained, no need to pass in a pre-existing embedding\n",
    "# Number of words + 1 do to unknown words being embedded as a whole\n",
    "# 1 Embedding per word, each being 32 feature values in length\n",
    "model.add(Embedding(MAXWORDS + 1, 32))\n",
    "\n",
    "# Bidirectional wrapper for neural layer networks, very important for Natural Language Processing (NLP)\n",
    "## Allows for information to be passed in both directions as opposed to the default 1\n",
    "## Important due to how previous words in a sentence can affect meaning of following words\n",
    "### Example: \"I don't have you.\" \n",
    "### If this was read only from left to right, the \"don't\" would not affect the interpretation of the word 'hate'\n",
    "### as a toxic value, as opposed to if it can be read from right to left as well\n",
    "# GPU acceleration needed for an LSTM layer is an activation of 'tanh' as dictated by tensorflow\n",
    "model.add(Bidirectional(LSTM(32, activation = 'tanh')))\n",
    "\n",
    "# Feature extractor dense fully connected layers\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "# Final layer\n",
    "## Maps to the 6 different outputs possible in terms of the target toxicity\n",
    "model.add(Dense(6, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17a414aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "[0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(commentsFeatures.shape)\n",
    "print(commentsFeatures[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "# Using binary crossentropy as opposed to categorical as each output is it's own independent feature\n",
    "## and not part of a whole, this is essentially a multi output model\n",
    "model.compile(loss = 'BinaryCrossentropy', optimizer = 'Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4587be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional doubles the number of units within the LSTM layer\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4212b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the deep neural network model\n",
    "## Passing in the training data, how long the desire to train and the validation data\n",
    "## Loss should be progressively decreasing as training nears completion\n",
    "model.fit(train, epochs = 10, validation_data = validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa461050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss metrics of model training\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ede48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting change in losses over training epochs\n",
    "plt.figure(figsize = (8, 5))\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419bb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "# The sample text has to be vectorized and the words tokenized for the model to be able to make sense of it\n",
    "sampleInputText = vectorizer('You freaking suck.')\n",
    "\n",
    "# A sequence of integers\n",
    "print(sampleInputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model expects a series of values or a batch, not a single value, so the vectorized \n",
    "## sample input has to be wrapped within a numpy array\n",
    "## The input shape for the model and the passed in value must match\n",
    "print(np.array([sampleInputText]))\n",
    "print(np.expand_dims(sampleInputText, 0))\n",
    "\n",
    "#model.predict(np.array([sampleInputText]))\n",
    "\n",
    "# Cleaner version than above example\n",
    "sampleResult = model.predict(np.expand_dims(sampleInputText, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c37cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = test.as_numpy_iterator().next()\n",
    "batchX, batchY = test.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ee3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model.predict(batchX) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating The Model\n",
    "# Allows for aggregation of metrics over time as the measurement are iterated over and over again over batches\n",
    "modelPrecision = Precision()\n",
    "modelRecall = Recall()\n",
    "modelCategoricalAccuracy = CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through every single batch\n",
    "for batch in test.as_numpy_iterator():\n",
    "    \n",
    "    # Unpacking the batch\n",
    "    xTrue, yTrue = batch\n",
    "    \n",
    "    # Making the precition\n",
    "    yHat = model.predict(xTrue)\n",
    "    \n",
    "    # Flattening the predictions into one very large vector\n",
    "    ## Instead of a 6x6 matrix, it becomes a 36x1\n",
    "    yTrue = yTrue.flatten()\n",
    "    yHat = yHat.flatten()\n",
    "    \n",
    "    # Calculating the metrics for the batch, then updating the existing Key Performance Indicators (KPIs)\n",
    "    modelPrecision.update_state(yTrue, yHat)\n",
    "    modelRecall.update_state(yTrue, yHat)\n",
    "    modelCategoricalAccuracy.update_state(yTrue, yHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prinintg Results\n",
    "print(f'Precision: {modelPrecision.result().numpy()}, Recall: {modelRecall.result().numpy()}, Accuracy: {modelCategoricalAccuracy.result().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac25b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving The Model\n",
    "model.save('commentToxicityModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f71acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading The Model\n",
    "model = tf.keras.models.load_model('commentToxicityModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2718af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing & Gradio Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to be connected to Gradio\n",
    "\n",
    "# A text comment is passed in as an argument\n",
    "def scoreCommentToxicity(commentText):\n",
    "    \n",
    "    # The comment is passed through the vectorizer, tokenizing the words\n",
    "    # Text is converted into a sequence of numbers\n",
    "    vectorizedComment = vectorize([commentText])\n",
    "    \n",
    "    # The tokenized comment is passed through the model to predict whether or not it is of a toxic nature\n",
    "    results = model.predict(vectorizedComment)\n",
    "    \n",
    "    text = ''\n",
    "    \n",
    "    # Unpacks the 'results' by looping through each of the dataframe's columns\n",
    "    # Prints out whether or not the column's feature is true or false for its associated comment\n",
    "    for index, column in enumerate(df.columns[2:]):\n",
    "        text += '{}: {}\\n'.format(column, results[0][index] > 0.5)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e71f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating The Gradio Interface\n",
    "## 'fn': The function the Gradio interface makes use of\n",
    "## 'inputs': The type of input\n",
    "## 'outputs': The style of output\n",
    "gradioInterface = gr.Interface(fn = scoreCommentToxicity,\n",
    "                               inputs = gr.inputs.Textbox(lines = 2, placeholder = 'Comment to be scored.'),\n",
    "                               outputs = text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launching The Interface\n",
    "## If the 'share' value is set to 'True', the gradio application will be made public for a limited amount of time\n",
    "gradioInterface.launch(share = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
